{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"e37cb69cb73a49c2ad07cf670e073cb7","deepnote_cell_height":156.390625,"deepnote_cell_type":"markdown","id":"XUZ1dFPHzAHl"},"source":["<h1><center>Laboratorio 4: Spark y EDA</center></h1>\n","\n","<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos - Primavera 2024</strong></center>"]},{"cell_type":"markdown","metadata":{"id":"PkEUN6c8S-E_"},"source":["### Cuerpo Docente:\n","\n","- Profesores: Ignacio Meza, Sebastián Tinoco\n","- Auxiliar: Eduardo Moya\n","- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"]},{"cell_type":"markdown","metadata":{"cell_id":"8ebcb0f2f70c43319279fdd28c13fe89","deepnote_cell_height":171.796875,"deepnote_cell_type":"markdown","id":"tXflExjqzAHr"},"source":["### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados\n","\n","- Nombre de alumno 1: Angelo León\n","- Nombre de alumno 2: Damián De Aguiar\n"]},{"cell_type":"markdown","metadata":{"cell_id":"290822720f3e4484b09e762655bcdb76","deepnote_cell_height":62,"deepnote_cell_type":"markdown","id":"AD-V0bbZzAHr"},"source":["### **Link de repositorio de GitHub:** [Repositorio](https://github.com/...../)"]},{"cell_type":"markdown","metadata":{"cell_id":"60255b81ff0349ad9b18f598a8d71386","deepnote_cell_height":216,"deepnote_cell_type":"markdown","id":"hnYD2hBMAwXf","tags":[]},"source":["## Reglas:\n","\n","- **Grupos de 2 personas**\n","- Fecha de entrega: 6 días de plazo con descuento de 1 punto por día. Entregas Jueves a las 23:59.\n","- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria.\n","- <u>Prohibidas las copias</u>. Cualquier intento de copia será debidamente penalizado con el reglamento de la escuela.\n","- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no estén en u-cursos no serán revisados. Recuerden que el repositorio también tiene nota.\n","- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n","- Pueden usar cualquier material del curso que estimen conveniente."]},{"cell_type":"markdown","metadata":{"cell_id":"5bf6f5f66dcd4da9a6926774cec108ab","deepnote_cell_height":114.390625,"deepnote_cell_type":"markdown","id":"xzz695obAwXg","tags":[]},"source":["### Temas a tratar\n","\n","- Introducción al manejo y análisis de grandes volúmenes de datos por medio de la libreria `pyspark`."]},{"cell_type":"markdown","metadata":{"cell_id":"50ec30f08f2548a29bc979ed1741f5a0","deepnote_cell_height":243.390625,"deepnote_cell_type":"markdown","id":"6uBLPj1PzAHs"},"source":["### Objetivos principales del laboratorio\n","\n","- Entender, aplicar y aprovechar las ventajas que nos ofrece la libreria `pyspark` para manejar datos tabulares de gran volúmen.\n","- Crear gráficos para el desarrollo de Análisis de Datos Exploratorios (EDA)."]},{"cell_type":"markdown","metadata":{"id":"f7hHEyTgm12s"},"source":["### Datos del Lab\n","\n","- Base de datos: https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/datos_lab_spark.parquet\n","- Objeto serializado: https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/object.pkl"]},{"cell_type":"markdown","metadata":{"id":"6CrDdk5NRAKe"},"source":["## 1. Preguntas Teóricas [12 puntos]\n","(2 por pregunta)"]},{"cell_type":"markdown","metadata":{"id":"EmDMGUTxLp7M"},"source":["<center>\n","<img src=\"https://img.buzzfeed.com/buzzfeed-static/static/2018-08/1/17/enhanced/buzzfeed-prod-web-05/anigif_enhanced-9173-1533160033-1.gif\" width=350 />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pGZZcxMWRdIa"},"source":["Responda en  máximo 5 líneas las siguientes preguntas:\n","1. ¿Qué es Apache Spark y cuáles son sus principales ventajas sobre Pandas?\n","2. ¿Qué es un RDD en Spark? Describe una de sus principales características. ¿Qué tienen que ver con los dataframes?.\n","3. Diferencia entre transformaciones y acciones en Spark. Proporciona ejemplos de cada una. ¿Qué ocurre internamente cuando se ejecuta una acción?\n","4. Explica la importancia del particionamiento en Spark y cómo afecta el rendimiento del procesamiento de datos.\n","5. ¿Cuáles son las funciones de Spark Driver y Spark Executor?\n","6. ¿Qué es el Catalyst Optimizer en Apache Spark y cuál es su función principal en la optimización de consultas SQL?\n"]},{"cell_type":"markdown","metadata":{"id":"1elJgE8JRn2O"},"source":["**Respuestas**\n","\n","> Respuestas aquí\n","Copy paste de gepeto:\n","Aquí están las respuestas a cada una de tus preguntas sobre Apache Spark:\n","\n","##### 1. ¿Qué es Apache Spark y cuáles son sus principales ventajas sobre Pandas?\n","\n","**Apache Spark** es un framework de procesamiento distribuido de código abierto diseñado para el procesamiento de grandes volúmenes de datos (Big Data). Se utiliza principalmente para computación en clústeres y permite realizar análisis de datos rápidos y distribuidos a gran escala.\n","\n","**Ventajas de Spark sobre Pandas**:\n","- **Escalabilidad**: Spark puede manejar datasets mucho más grandes que Pandas, ya que distribuye el procesamiento entre múltiples nodos en un clúster, mientras que Pandas es más adecuado para datos que caben en memoria.\n","- **Procesamiento distribuido**: Spark permite el procesamiento en paralelo en múltiples máquinas, lo que mejora el rendimiento para grandes volúmenes de datos, mientras que Pandas es más adecuado para un solo nodo.\n","- **Tolerancia a fallos**: Spark tiene mecanismos para gestionar fallos en el procesamiento distribuido, mientras que Pandas depende de la estabilidad del hardware local.\n","- **Almacenamiento en memoria**: Spark optimiza el procesamiento al almacenar datos intermedios en memoria, lo que mejora el rendimiento, mientras que Pandas solo utiliza la memoria local y puede volverse ineficiente con grandes datasets.\n","\n","##### 2. ¿Qué es un RDD en Spark? Describe una de sus principales características. ¿Qué tienen que ver con los DataFrames?\n","\n","**RDD (Resilient Distributed Dataset)** es la estructura de datos fundamental en Spark que representa una colección inmutable de objetos distribuidos en un clúster. Los RDD permiten el procesamiento de datos de manera distribuida y eficiente.\n","\n","Una de sus principales características es que son **inmutables**, lo que significa que no se pueden modificar una vez creados. En lugar de modificar un RDD, se pueden crear nuevos RDD a partir de transformaciones en los datos originales.\n","\n","En relación con los **DataFrames**, estos son una abstracción más avanzada y eficiente que los RDD. Los DataFrames están optimizados para operaciones tabulares y ofrecen un rendimiento superior gracias al uso del Catalyst Optimizer y a la ejecución más optimizada para las consultas SQL. Los DataFrames son, internamente, una capa de abstracción sobre los RDD, que permite un procesamiento más fácil y rápido de los datos.\n","\n","##### 3. Diferencia entre transformaciones y acciones en Spark. Proporciona ejemplos de cada una. ¿Qué ocurre internamente cuando se ejecuta una acción?\n","\n","- **Transformaciones**: Son operaciones que definen un nuevo RDD a partir de uno existente. Las transformaciones son *lazy*, lo que significa que no se ejecutan inmediatamente, sino que se acumulan en un *plan de ejecución* hasta que se llama una acción. Ejemplos:\n","  - `map()`: Transforma cada elemento de un RDD según una función.\n","  - `filter()`: Filtra los elementos de un RDD según un criterio.\n","\n","- **Acciones**: Son operaciones que devuelven un valor concreto después de procesar el RDD, desencadenando la ejecución de las transformaciones acumuladas. Ejemplos:\n","  - `collect()`: Devuelve todos los elementos del RDD al driver.\n","  - `count()`: Devuelve el número de elementos en el RDD.\n","\n","Internamente, cuando se ejecuta una acción, Spark **construye el DAG (Directed Acyclic Graph)** de las transformaciones y las ejecuta en los nodos del clúster, paralelizando el procesamiento y aplicando optimizaciones como la combinación de operaciones.\n","\n","##### 4. Explica la importancia del particionamiento en Spark y cómo afecta el rendimiento del procesamiento de datos.\n","\n","El **particionamiento** en Spark es crucial porque determina cómo se dividen y distribuyen los datos entre los nodos del clúster. Un particionamiento adecuado permite paralelizar eficientemente las tareas, mientras que un mal particionamiento puede generar cuellos de botella.\n","\n","- **Impacto en el rendimiento**:\n","  - Un **particionamiento insuficiente** (demasiados datos en pocas particiones) sobrecarga ciertos nodos, ralentizando el procesamiento.\n","  - Un **particionamiento excesivo** (demasiadas particiones pequeñas) provoca una sobrecarga de tareas y comunicación entre nodos, lo que también reduce el rendimiento.\n","  \n","   La clave es equilibrar el número de particiones con la cantidad de datos y los recursos del clúster. Funciones como `repartition()` o `coalesce()` se utilizan para ajustar las particiones de los RDD o DataFrames.\n","\n","##### 5. ¿Cuáles son las funciones de Spark Driver y Spark Executor?\n","\n","- **Spark Driver**: Es el componente principal que coordina todo el trabajo en una aplicación Spark. Su función principal es:\n","  - Mantener la **lógica del programa** Spark.\n","  - **Distribuir tareas** a los ejecutores en los nodos del clúster.\n","  - **Rastrear el progreso** de las tareas y recopilar los resultados.\n","\n","- **Spark Executor**: Son procesos que se ejecutan en los nodos de trabajo del clúster y llevan a cabo las tareas que les asigna el driver. Sus funciones principales son:\n","  - **Ejecutar las tareas** asignadas sobre los datos particionados.\n","  - **Almacenar los datos intermedios** en memoria o disco según sea necesario.\n","  - **Enviar los resultados** de vuelta al driver.\n","\n","##### 6. ¿Qué es el Catalyst Optimizer en Apache Spark y cuál es su función principal en la optimización de consultas SQL?\n","\n","El **Catalyst Optimizer** es el motor de optimización de consultas SQL en Spark. Su función principal es optimizar los planes de ejecución que se generan cuando se ejecutan consultas SQL o se realizan operaciones sobre DataFrames.\n","\n","El Catalyst Optimizer realiza varias optimizaciones, como:\n","- **Análisis lógico**: Comprende el tipo de datos y las operaciones que se desean realizar.\n","- **Optimización del plan lógico**: Simplifica las consultas eliminando redundancias o aplicando reglas de optimización como la selección de proyecciones más eficientes.\n","- **Generación de un plan físico**: Determina cómo ejecutar las operaciones en el clúster.\n","- **Optimización basada en coste**: Evalúa diferentes estrategias de ejecución para seleccionar la que tenga un mejor rendimiento.\n","\n","Gracias al Catalyst Optimizer, las operaciones sobre DataFrames y SQL en Spark son mucho más eficientes que las operaciones sobre RDDs.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"00002-bf13ea5a-d8bf-4cee-879e-ba1c7035e657","deepnote_cell_type":"markdown","id":"b020ce37"},"source":["## Parte Práctica\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k0DaDvtgEYTV"},"source":["<center>\n","<img src=\"https://pbs.twimg.com/ad_img/1285681293590749189/kDckYy6Z?format=png&name=900x900\" width=350 />"]},{"cell_type":"markdown","metadata":{"id":"uW1dg_5_WR8S"},"source":["Juan Carlos Bodoque, el famoso periodista y empresario, decidió diversificar su portafolio de negocios y crear su propia plataforma de e-commerce. Después de varios años de investigar y analizar el mercado financiero, finalmente logró fundar Bodoque E-Shop con el objetivo de ofrecer a sus clientes una experiencia personalizada y confiable en sus transacciones.\n","\n","Sin embargo, con la llegada de los aliens al planeta Tierra, aparecen nuevos desafíos para el negocio. Por ello, Bodoque decide invertir en un equipo de expertos en tecnología y comercio interplanetario, para que Bodoque Shop implemente las últimas innovaciones en servicio al cliente para garantizar la satisfacción y fidelización de sus nuevos clientes.\n","\n","El primer objetivo de Bodoque E-Shop será la hacer un análisis exploratorio para entender mejor el comportamiento de los usuarios en la plataforma. Para ello Bodoque les hace entrega de un extenso dataset en el que se registran las actividades que han realizado sus clientes durante los últimos meses. A continuación se presenta un diccionario de variables que levanto el equipo de consultores interplanetarios de Bodoque:\n","\n","1. `Transaction ID`: A unique identifier for each transaction.\n","2. `Customer ID`: A unique identifier for each customer.\n","3. `Transaction Amount`: The total amount of money exchanged in the transaction in USD.\n","4. `Transaction Date`: The date and time when the transaction took place.\n","5. `Payment Method`: The method used to complete the transaction (e.g., credit card, PayPal, etc.).\n","6. `Product Category`: The category of the product involved in the transaction.\n","7. `Quantity`: The number of products involved in the transaction.\n","8. `Customer Age`: The age of the customer making the transaction.\n","9. `Customer Location`: The geographical location of the customer.\n","10. `Device Used`: The type of device used to make the transaction (e.g., mobile, desktop).\n","11. `IP Address`: The IP address of the device used for the transaction.\n","Shipping Address: The address where the product was shipped.\n","12. `Billing Address`: The address associated with the payment method.\n","13. `Is An Alien`: A binary indicator of whether customer is an alien.\n","14. `Account Age Days`: The age of the customer's account in days at the time of the transaction.\n","15. `Transaction Hour`: The hour of the day when the transaction occurred.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"1769820f70244385ab5ac51f7509b6de","deepnote_cell_height":61.133331298828125,"deepnote_cell_type":"markdown","id":"MhISwri4zAHy"},"source":["### Importamos librerias utiles y cargamos los datos😸"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xHoq7VBlJoS3"},"outputs":[],"source":["# Todas las cosas coon pip las tienes que instalar en el terminal antes de ejecutar el código\n","#!pip install pyspark\n","#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"M6MKzLmPSHzY"},"outputs":[],"source":["# Libreria Core del lab.\n","import pyspark\n","from pyspark import SparkConf, SparkContext\n","from pyspark.sql import SparkSession\n","import pandas as pd\n","from pyspark.sql.types import StringType, IntegerType, FloatType\n","\n","\n","#Libreria para plotear\n","#!pip install --upgrade plotly\n","#!pip install missingno\n","import matplotlib.pyplot as plt\n","import plotly.express as px"]},{"cell_type":"markdown","metadata":{"id":"9vJWSlEXYBqq"},"source":["Cargue los datos usando **pyspark**\n","\n","> Nota: Puede ser util el siguiente [enlace](https://www.oracle.com/cl/java/technologies/downloads/#jdk22-windows)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Inicializar SparkSession\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCargarParquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Cargar el archivo Parquet\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df_parquet \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~/labPrograCientifica/Lab4/datos_lab_spark.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\crrfo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n","File \u001b[1;32mc:\\Users\\crrfo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n","File \u001b[1;32mc:\\Users\\crrfo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\crrfo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n","File \u001b[1;32mc:\\Users\\crrfo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\java_gateway.py:104\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 104\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Carga archivo parquet:\n","\n","from pyspark.sql import SparkSession\n","\n","# Inicializar SparkSession\n","spark = SparkSession.builder.appName(\"CargarParquet\").getOrCreate()\n","\n","# Cargar el archivo Parquet\n","df_parquet = spark.read.parquet(\"~/labPrograCientifica/Lab4/datos_lab_spark.parquet\")\n","\n","# Mostrar los primeros registros\n","df_parquet.show()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"i9Uf-BTZXqXe"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '~/labPrograCientifica/Lab4/archivo.parquet'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url_parquet, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Guardar el archivo localmente\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m~/labPrograCientifica/Lab4/archivo.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     13\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(r\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchivo Parquet descargado con éxito.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/labPrograCientifica/Lab4/archivo.parquet'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"z6l6GNynYnh4"},"source":["### 2. Limpieza con pyspark [8 puntos]\n","(1 punto por pregunta)"]},{"cell_type":"markdown","metadata":{"id":"8DVdjYyOGRom"},"source":["<center>\n","<img src=\"https://miro.medium.com/v2/resize:fit:600/1*A6PpTrehGLxCJWNcUsDTNg.jpeg\" width=350 />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sPGV40BjZekP"},"source":["Para comenzar con el análisis exploratorio usted decide empezar limpiando la base de datos con **pyspark** dado el alto volumen de datos que genera diariamente Bodoque E-Shop.\n","\n","**Nota: NO SE PERMITE EL USO DE PANDAS EN ESTA SECCIÓN**\n","\n","\n","\n","1.   Utilice `.printSchema()` para revisar la estructura de los datos\n","2.   Muestre las primeras 10 filas del dataset. Hint: utilice `.show()`\n","3.   Imprima un muestreo aleatorio con el 5% de los datos diponibles. . Hint: utilice `.sample()`\n","4. Revise los tipos de datos de cada columna con `.dtypes()` y responda la siguiente pregunta: ¿Cuál/es columna/s tiene/n un tipo de dato que no es el adecuado y por qué?\n","5. Cree una función **cast_columns** que permita cambiar el tipo de datos de las columnas problemáticas. Luego utilice esta función respecto a lo respondido en la pregunta anterior.\n","6. Cuente la cantidad de datos nulos por variable. Recuerde que Spark no posee un método que le permita calcular directamente los nulos.\n","7. Elimine datos nulos.\n","8. Elimine datos duplicados.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nw95Jvr-DtwS"},"outputs":[],"source":["# Escriba su respuesta aquí"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def cast_columns(df, cols_types):\n","    \"\"\"\n","    Cambia el tipo de múltiples columnas en un DataFrame de Spark.\n","\n","    Parámetros:\n","    - df: DataFrame de Spark.\n","    - cols_types: Diccionario con nombres de columnas como claves y tipos de datos de Spark como valores.\n","\n","    Retorna:\n","    - DataFrame de Spark con tipos de columnas modificados.\n","    \"\"\"\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"MjxI2Xd6cRu1"},"source":["### 3. Transformaciones con pyspark [6 puntos]\n","(1 punto por pregunta)"]},{"cell_type":"markdown","metadata":{"id":"bPfhWPZeHXUH"},"source":["<center>\n","<img src=\"https://live.staticflickr.com/13/91801406_0e71d7f019_b.jpg\" width=350 />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lbIDKn44cWhI"},"source":["**Nota: NO SE PERMITE EL USO DE PANDAS EN ESTA SECCIÓN**\n","\n","Para continuar con el análisis, los especistas de Bodoque les gustaría tener nuevas variables disponibles. Tras las notas de la reunión usted llega a la conclusión de que tiene que realizar las siguientes tareas (con el dataset preprocesado de la seccion anterior):\n","\n","\n","1.   Agregar una columna llamada \"Transaction bp\" con el **monto total** de la transacción en bodoque pesos. Se considera que $x$ dólares equivalen a $log(48+|x^{36}|)$ bodoque pesos.\n","2.   Crear una columna llamada \"Transaction Month\" con el mes en que se realiza una transacción.\n","3.   Crear la variable *Type of purchase* según la catidad de unidades vendidas de acuerdo a las siguientes categorías.\n","  * Compra minorista: 4 productos o menos.\n","  * Compra mayorista: 5 produtos o más.\n","4. Imprima los registros de compras hechas por alienígenas en el comecio mayorista.  Utilice `.filter()`.\n","5. Cuente la cantidad de compras realizadas por humanos y la cantidad de compras realizadas por alienígenas. Utilice `.groupby()`.\n","6. Muestre una tabla con la recaudación promedio por transacción para cada método de pago, tanto para humanos como alienígenas. Utilice `pivot()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbtFJi3mHnkK"},"outputs":[],"source":["# Escriba su respuesta aquí"]},{"cell_type":"markdown","metadata":{"id":"17Muj6u2jOLq"},"source":["### 4. EDA [20 puntos]\n","(1 punto por gráfico y 1 punto por su interpretación)"]},{"cell_type":"markdown","metadata":{"id":"7F3yo66wFQ0z"},"source":["<center>\n","<img src=\"https://i.pinimg.com/originals/41/7e/7b/417e7b9089bcc20c4909df8954c6e742.gif\" width=400 />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ayN5LYRamE7-"},"source":["Esta sección tiene como objetivo evaluar su habilidad para generar reportes y conclusiones a partir de los patrones identificados en los datos proporcionados por Bodoque. Específicamente, se enfoca en **caracterizar las transacciones** y **explorar las diferencias y similitudes en el comportamiento de humanos y aliens**. Utilice el dataset que ya incluye las transformaciones necesarias.\n","\n","Por favor, asegúrese de que **todas** las visualizaciones que realice cumplan con los siguientes criterios:\n","- Deben ser relevantes y fáciles de interpretar.\n","- Cada gráfico debe incluir un título claro, nombres en los ejes y leyendas adecuadas.\n","- Adjunte una breve descripción interpretativa junto a cada gráfico para explicar los resultados visualizados.\n","\n","Para llevar a cabo esta tarea, siga los siguientes pasos utilizando la librería de visualización de su elección (matplotlib, seaborn, plotly, etc):\n","\n","1. **Conversión del DataFrame a formato pandas** (2 puntos): Pase el DataFrame procesado a formato pandas. Evite realizar transformaciones adicionales con pandas.\n","2. **Visualización de Variables Categóricas** (2 puntos por visualización):\n","   - Genere **tres gráficos de barras** que diferencien entre humanos y aliens. Analice y comente cualquier diferencia o similitud observada entre estos dos grupos.\n","3. **Visualización de Variables Numéricas** (2 puntos por visualización):\n","   - Elabore **tres distplots** para examinar las distribuciones de variables numéricas, diferenciando entre humanos y aliens. Comente las diferencias o similitudes notables.\n","4. **Análisis de Patrones en Transacciones** (2 puntos por visualización):\n","   - Cree **tres gráficos avanzados** que ayuden a identificar patrones en las transacciones. Estos gráficos deben incorporar al menos dos dimensiones y diferir de los anteriores. Algunos ejemplos podrían ser un lineplot que muestre la cantidad de transacciones mensuales por canal de venta, o un barplot que exhiba los tres productos más vendidos por canal.\n","\n","Estos pasos le permitirán no solo visualizar datos complejos de manera efectiva, sino también interpretar estos datos para extraer insights valiosos acerca del comportamiento de los consumidores en el contexto de Bodoque."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGw5y36IxRk3"},"outputs":[],"source":["# Escriba su respuesta aquí"]},{"cell_type":"markdown","metadata":{"id":"97zN2_g4vgY6"},"source":["### 5. Particiones y consultas en SQL [2 puntos]"]},{"cell_type":"markdown","metadata":{"id":"viNvNuE_odgc"},"source":["<center>\n","<img src=\"https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/1696330143457.gif\" width=400 />"]},{"cell_type":"markdown","metadata":{"id":"SCdHwyGBwVx8"},"source":["El equipo de Bodoque e-shop ha solicitado que los datos estén disponibles en una tabla SQL consultable. Además, están interesados en aprovechar las funciones de ventana en SQL para análisis avanzados. Las funciones de ventana permiten realizar cálculos sobre un conjunto de filas que están relacionadas con la fila actual. Por ejemplo, UNBOUNDED PRECEDING se usa para indicar que el rango de la función de ventana comienza desde la primera fila de la partición o del conjunto de resultados, lo cual es útil para calcular sumas acumulativas hasta la fila actual. Las variaciones comunes de este uso incluyen:\n","\n","- `UNBOUNDED PRECEDING` to `CURRENT ROW`: Calcula desde el inicio de la partición hasta la fila actual.\n","- `UNBOUNDED PRECEDING` to `UNBOUNDED FOLLOWING`: Cubre todas las filas dentro de la partición.\n","- `VALUE PRECEDING` to `VALUE FOLLOWING`: Establece un rango específico basado en valores antes y después de la fila actual."]},{"cell_type":"markdown","metadata":{"id":"VntjejKLleIa"},"source":["<center>\n","<img src=\"https://learnsql.com/blog/sql-window-functions-rows-clause/1.png\" width=500 />"]},{"cell_type":"markdown","metadata":{"id":"D8XJ7NrPllKG"},"source":["Ejemplo de uso en SQL:\n","\n","```sql\n","STAT(COL1_NAME) OVER (PARTITION BY COL2_NAME ORDER BY COL3_NAME ROWS BETWEEN X PRECEDING AND CURRENT ROW)\n","```\n","\n","\n","Responda y realice los siguientes puntos:\n","\n","1. **Creación de Tabla con PySpark** (2 puntos):\n","   - Desarrolle un script utilizando PySpark para crear una tabla a partir de un DataFrame previamente transformado. Seleccione y utilice una variable específica para la partición de la tabla. Justifique su elección de esta variable considerando factores como el tamaño del DataFrame, la distribución de los datos y el impacto potencial en el rendimiento de futuras consultas.\n","\n","2. **Consulta SQL para Principales Clientes** (Bonus: 2 punto):\n","   - Ejecute una consulta SQL para identificar los 10 clientes que más productos han comprado. La consulta debe retornar el ID del cliente junto con el total de productos comprados, ordenados en forma descendente.\n","\n","3. **Implementación de Función de Ventana en SQL y Equivalente en Spark** (Bonus: 2 punto):\n","   - Implemente una función de ventana en SQL para calcular la compra más alta realizada por cada usuario en los últimos tres meses. Además, describa cómo se podría realizar una función equivalente en Spark, considerando las capacidades específicas de PySpark para manejar este tipo de consultas.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xe_JQ3npiM6_"},"outputs":[],"source":["# Código Aquí"]},{"cell_type":"markdown","metadata":{"id":"AKQs-augfZBv"},"source":["### 6. UDF [12 puntos]"]},{"cell_type":"markdown","metadata":{"id":"ovDBGi-uhhdD"},"source":["<center>\n","<img src=\"https://64.media.tumblr.com/ba8c705edd2bed0a28d9458811155d69/tumblr_pap19zg4ae1w3zg6go1_400.gifv\" width=400 />"]},{"cell_type":"markdown","metadata":{"id":"TJUUnpi8qKHD"},"source":["\n","\n","Un experto en predicciones y programación le ha proporcionado un objeto serializado (`pickle`) diseñado para calcular las probabilidades de que un cliente cometa o no un fraude. Este experto sugiere que, para maximizar las capacidades de procesamiento distribuido de Spark, debería implementar `Scalar User Defined Functions` (udf). Esto le permitirá aplicar el objeto serializado en un entorno distribuido a lo largo de toda la población de datos. Un aspecto clave de la función desarrollada por el experto es que se enfoca exclusivamente en las siguientes columnas para realizar las predicciones: `['Transaction Amount', 'Quantity', 'Customer Age', 'Transaction Hour']`.\n","\n","Aparte, el experto le proporciona las siguientes instrucciones para usar las UDF en Spark:\n","\n","```python\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import FloatType\n","\n","def custom_function(col1, col2, col3, col4):\n","    pass\n","\n","udf_function = udf(custom_function, FloatType())\n","```\n","\n","Basándose en la estructura proporcionada, debe desarrollar una función que ejecute un código específico. Tenga en cuenta que esta función solo puede recibir columnas de Spark y debe retornar el valor deseado. Posteriormente, deberá utilizar esta función UDF indicando la función personalizada y el formato de salida.\n","\n","Siga los siguientes pasos para implementar la solución y responda las preguntas:\n","\n","1. **Cargar el objeto serializado**: Revise el tipo de objeto y deduzca su función. (1 punto)\n","2. **Explorar el objeto**: Utilice las funciones `dir` y `help` para identificar qué método del objeto predice la probabilidad. (1 punto)\n","3. **Crear una función personalizada**: Elabore una función que prediga la probabilidad de fraude utilizando el último valor de la lista generada por el objeto serializado. Puede modificar el nombre de la función para reflejar su propósito. (6 puntos)\n","4. **Definir la función UDF**: Establezca la función UDF con la función personalizada que ha creado. (2 punto)\n","5. **Generar una nueva columna**: Añada una nueva columna `prediction` a su DataFrame en Spark utilizando la función UDF y muestre un ejemplo de cómo se aplica. ¿Qué beneficios podría generar utilizar udf? (2 puntos)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPJVs2OBezN_"},"outputs":[],"source":["# Código Aquí"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
